---
title: "krr"
author: "Lu Cheng"
date: "4/17/2020"
output: pdf_document
---

```{r}
if(!require("remotes")){
  install.packages("remotes")
}

if(!require("krr")){
  remotes::install_github("TimothyKBook/krr")
}

if(!require("tidyverse")){
  install.packages("tidyverse")
}

if(!require("caret")){
  install.packages("caret")
}



library(tidyverse)
library(krr)
library(caret)

source("../lib/Matrix_Factorization.R")
source("../lib/cross_validation.R")
```


```{r}
result <- gradesc(f = 10, lambda = 0.1,lrate = 0.01, max.iter = 100, stopping.deriv = 0.01,
                   data = data, train = data_train, test = data_test)

# save(result, file = "../output/mat_fac.RData")

write.csv(result$p, file = "../output/p.csv")
write.csv(result$q, file = "../output/q.csv")
write.csv(result$b_user, file = "../output/b_user.csv")
write.csv(result$b_movie, file = "../output/b_movie.csv")
write.csv(result$b_bin, file = "../output/b_bin.csv")
write.csv(result$mu, file = "../output/mu.csv")


```

### Step 3 Postprocessing    
After matrix factorization, postporcessing will be performed to improve accuracy.    
#### Postprocessing SVD with Kernel Ridge Regression     
      
import data
```{r}
p <- read.csv("../output/p.csv")
q <- read.csv("../output/q.csv", header=F) 
# mu <- read.csv("../output/mu.csv")
# b_user <- read.csv("../output/b_user.csv")
# b_movie <- read.csv("../output/b_movie.csv")
# b_bin <- read.csv("../output/b_bin.csv")
rating <- read.csv("../data/ml-latest-small/ratings.csv")


```
     
normalize each vector $q_i$, i.e.$\frac{q_i}{||q_i||}$    
merge 'rating' and 'q_trans_normalized' datasets by movieId
```{r}
q_trans <- t(q[-1,-1])

normal <- function(a){return(a / sqrt(sum(a^2)))}
q_trans_normalized <- apply(q_trans, 2, normal)
rownames(q_trans_normalized) <- 1:nrow(q_trans_normalized)
colnames(q_trans_normalized) <- 1:ncol(q_trans_normalized)

# separate movieId from q dataset
movieId <- as.integer(as.vector(unlist(c(q[1,])))[-1])

#combine movieId with q_trans_normalized dataset
q_trans_normalized <- cbind(movieId, q_trans_normalized)
q_trans_normalized <- as_tibble(q_trans_normalized)

rating_merge <- rating %>% 
  as_tibble() %>%
  left_join(q_trans_normalized, by = c('movieId' = 'movieId')) %>%
  mutate(movieId = as.integer(movieId))

rating_merge
```
    
split 'rating_mearge' dataset into train and test datasets
```{r}
set.seed(0)

test_idx <- sample(1:nrow(rating_merge), round(nrow(rating_merge)/5, 0))
train_idx <- setdiff(1:nrow(rating_merge), test_idx)

# 80% training
train_data <- rating_merge[train_idx,]
#20% testing
test_data <- rating_merge[test_idx,]

#number of users
n_users <- length(unique(rating_merge$userId))


#split data for 610 users
test_split <- list()
for (i in 1:n_users){
  test_split[[i]] <- cbind(test_data[test_data$userId == i, 'rating'],
                      test_data[test_data$userId == i, 5:dim(test_data)[2]])
}

train_split <- list()
for (i in 1:n_users){
  train_split[[i]] <- cbind(train_data[train_data$userId == i, 'rating'],
                      train_data[train_data$userId == i, 5:dim(train_data)[2]])
}

```   
    
##### Tuning parameter for krr    
set the kernel as Gaussian as Paper2 said and use cross validation to get a smaller RMSE.
write a function to do cross validation for parameters lambda and sigma in krr
```{r}
cv.krr <- function(data, kfold, lambda, sigma){
  
  set.seed(123)
  cv_mean <- c()
  cv_tuning <- c()
  for (i in 1:n_users){
    x <- as.matrix(data[[i]][, -1])
    y <- as.matrix(data[[i]][, 1])
    n <- nrow(x)
    cv_id <- createFolds(1:n, k = kfold)
    
    for (j in cv_id){
    #Split Data in train and validation sets
      x_train_cv <- x[-j,]
      y_train_cv <- y[-j]
      x_validation_cv <- x[j,]
      y_validation_cv <- y[j]
    
    #Run Model
      model_cv <- krr(x_train_cv, y_train_cv, lambda = lambda, sigma = sigma)
    #Estimate predictin of validation test
      pred_cv <- predict(model_cv, x_validation_cv)
    
    #Calculate RMSE
      rmse_cv <- sqrt(mean((y_validation_cv - pred_cv)^2))
      cv_tuning <- append(cv_tuning, rmse_cv)
      
    }
    cv_mean <- append(cv_mean, mean(cv_tuning))
  }  
  return(cv_mean)
}
```
   
Find the best parameters that get a smaller rmse
```{r}
lambdas <- c(4.0, 5.0, 6.0)
sigmas <- c(1, 2)
# rmse_tune <- data.frame(lambdas = numeric(), 
#                         sigmas = numeric(),
#                         rmse = numeric())
rmse_tune <- data.frame()
# colnames(rmse_tune) <- c("lambda", "sigma", "rmse")

for (i in 1:length(lambdas)){
  for (j in 1:length(sigmas)){
    m <- cv.krr(train_split, 5, lambdas[i], sigmas[j])
    rmse <-  sum(m)
    rmse_tune <- rbind(rmse_tune, c(lambdas[i], sigmas[j], rmse))
  }
  
}
colnames(rmse_tune) <- c("lambda", "sigma", "rmse_tune")

min_rmse <- rmse_tune %>%
  filter(rmse == min(rmse))
best_para <- min_rmse[,1:2]



best_para



```
lambda = 4, sigma = 2     
        
        
use the best lambda and sigma to train 610 kernel ridge regression models.
```{r}
t0 <- Sys.time()

train_model <- vector(mode="list",length = n_users)
for(i in 1:n_users){
  train_model[[i]] = krr(as.matrix(train_split[[i]][, -1]), 
                         as.matrix(train_split[[i]][, 1]),
                         min_rmse$lambda, min_rmse$sigma)
}

t1 <- Sys.time()

training_time <- t1 - t0
training_time


```
    
compute rmse
```{r}
<<<<<<< HEAD
rmse.fn <- function(data){
  error <- c()
  for (i in 1:n_users){
    X <- as.matrix(data[data$userId == i, 5:dim(data)[2]])
    y <- as.matrix(data[data$userId == i, 'rating'])
    y_pred <- predict(train_model[[i]], X)
    error <- append(error, (y - y_pred)^2)
  }

  return(sqrt(sum(error) / nrow(data)))

}

=======


error <- c()
for (i in 1:n_users){
  X <- as.matrix(test_data[test_data$userId == i, 5:dim(test_data)[2]])
  y <- as.matrix(test_data[test_data$userId == i, 'rating'])
  y_pred <- predict(train_model[[i]], X)
  error <- append(error, (y - y_pred)^2)
}

sqrt(sum(error) / nrow(test_data))
>>>>>>> 0c4df1b5dfa347914d50de5551fa790da7d0f871

test_rmse <- rmse.fn(test_data)
train_rmse <- rmse.fn(train_data)

<<<<<<< HEAD
test_rmse
train_rmse


```

test rmse is 0.9496269   
train rmse is 0.9429299
=======
test rmse is 0.95959
>>>>>>> 0c4df1b5dfa347914d50de5551fa790da7d0f871


